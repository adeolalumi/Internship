{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aeaaff35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia\\n\\nThe Free Encyclopedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1,000,000+\\n\\n\\narticles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100,000+\\n\\n\\narticles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10,000+\\n\\n\\narticles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1,000+\\n\\n\\narticles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100+\\n\\n\\narticles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Wikipedia\n",
       "0  Wikipedia\\n\\nThe Free Encyclopedia\n",
       "1            1,000,000+\\n\\n\\narticles\n",
       "2              100,000+\\n\\n\\narticles\n",
       "3               10,000+\\n\\n\\narticles\n",
       "4                1,000+\\n\\n\\narticles\n",
       "5                  100+\\n\\n\\narticles"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Question 1 Write a python program to display all the header tags from wikipedia.org and make data frame.'''\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'http://www.wikipedia.org'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all header tags (h1, h2, h3, etc.)\n",
    "header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "\n",
    "# Create a DataFrame with each header text in a separate row\n",
    "df = pd.DataFrame({'Wikipedia': [tag.text.strip() for tag in header_tags]})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9740e98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Men's Test Team Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Men's ODI Team Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Men's T20I Team Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Men's Test Batting Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Men's ODI Batting Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Men's T20I Batting Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Men's Test Bowling Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Men's ODI Bowling Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Men's T20I Bowling Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Men's Test All-Rounder Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Men's ODI All-Rounder Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Men's T20I All-Rounder Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Women's T20I Team Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Women's ODI Team Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Women's T20I Batting Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Women's ODI Batting Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Womens T20I Bowling Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Womens ODI Bowling Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Women's T20I ALL-Rounder Rankings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Women's ODI ALL-Rounder Rankings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Ranking Links\n",
       "0            Men's Test Team Rankings\n",
       "1             Men's ODI Team Rankings\n",
       "2            Men's T20I Team Rankings\n",
       "3         Men's Test Batting Rankings\n",
       "4          Men's ODI Batting Rankings\n",
       "5         Men's T20I Batting Rankings\n",
       "6         Men's Test Bowling Rankings\n",
       "7          Men's ODI Bowling Rankings\n",
       "8         Men's T20I Bowling Rankings\n",
       "9     Men's Test All-Rounder Rankings\n",
       "10     Men's ODI All-Rounder Rankings\n",
       "11    Men's T20I All-Rounder Rankings\n",
       "12         Women's T20I Team Rankings\n",
       "13          Women's ODI Team Rankings\n",
       "14      Women's T20I Batting Rankings\n",
       "15       Women's ODI Batting Rankings\n",
       "16       Womens T20I Bowling Rankings\n",
       "17        Womens ODI Bowling Rankings\n",
       "18  Women's T20I ALL-Rounder Rankings\n",
       "19   Women's ODI ALL-Rounder Rankings"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Question 2 Write s python program to display list of respected team rank from\n",
    "https://www.icc-cricket.com/rankings/team-rankings/mens/odi \n",
    "and make data frame.'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/team-rankings/mens/odi'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all anchor tags with the specified class\n",
    "mens_ranking_links = soup.find_all('a', class_='category-nav')\n",
    "\n",
    "# Initialize an empty list to store cleaned text\n",
    "cleaned_text_list = []\n",
    "\n",
    "# Iterate through the links and append cleaned text to the list\n",
    "for link in mens_ranking_links:\n",
    "    cleaned_text = link.text.strip()\n",
    "    cleaned_text_list.append(cleaned_text)\n",
    "\n",
    "# Create a DataFrame with each cleaned text in a separate column\n",
    "df = pd.DataFrame({'Ranking Links': cleaned_text_list})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e65b85c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The stock market has cooled off to start 2024,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bitcoinâ€™s post-ETF launch sell-off could conti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The consumer will be the key next week with re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dow closes more than 100 points lower Friday, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here are Wall Street's top semiconductor stock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Goldman says it's time to buy this unloved glo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Here's why Buffett calls airline stocks hazard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bitcoin slid after key ETFs debuted. What it m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Goldman picks China stocks in case there's a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pacific Islands lash out at COP28 presidency: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What the COP28 deal means for the U.S. amid re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>COP28 climate summit ends with deal to transit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Anger and frustration as COP28 draft text omit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A â€˜thirstyâ€™ generative AI boom poses a growing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ukraine says it needs China for any peace proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ukraine says China needed for peace process af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>U.S. Ukraine assistance 'ground to a halt'; Ze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ukraine and Russia both say they want peace. E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sanctioned Western tech is still entering Russ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bahamas PM fears climate action could be sidel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Earth close to breaching 1.5 degrees Celsius a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Norway poised to open vast ocean area to contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Oil industry veteran to lead next round of UN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CNBCâ€™s Sustainable Future Forum 2023: Watch th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Southeast Asia is on the cusp of a 'supercharg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cambodia's deputy prime minister: BRI has help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Southeast Asia's first luxury hotel made from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ahead of Indonesiaâ€™s elections, critics slam J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Laos is spiraling toward a debt crisis as Chin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Americans are canceling trips that are thousan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Social media uproar may cost the Maldives mill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>An new train pass is launching in Japan â€” but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Will air fares remain high in 2024? Here's wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Why foreign workers are flocking to this 700-y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>You are 48% more likely to land a first date i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>How 27-year-old whose business brought in $1 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The 'holy grail' of longevity foods this docto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The No. 1 in-demand remote job companies are h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The ultimate guide to acing your interview and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline\n",
       "0   The stock market has cooled off to start 2024,...\n",
       "1   Bitcoinâ€™s post-ETF launch sell-off could conti...\n",
       "2   The consumer will be the key next week with re...\n",
       "3   Dow closes more than 100 points lower Friday, ...\n",
       "4   Here are Wall Street's top semiconductor stock...\n",
       "5   Goldman says it's time to buy this unloved glo...\n",
       "6   Here's why Buffett calls airline stocks hazard...\n",
       "7   Bitcoin slid after key ETFs debuted. What it m...\n",
       "8   Goldman picks China stocks in case there's a r...\n",
       "9   Pacific Islands lash out at COP28 presidency: ...\n",
       "10  What the COP28 deal means for the U.S. amid re...\n",
       "11  COP28 climate summit ends with deal to transit...\n",
       "12  Anger and frustration as COP28 draft text omit...\n",
       "13  A â€˜thirstyâ€™ generative AI boom poses a growing...\n",
       "14  Ukraine says it needs China for any peace proc...\n",
       "15  Ukraine says China needed for peace process af...\n",
       "16  U.S. Ukraine assistance 'ground to a halt'; Ze...\n",
       "17  Ukraine and Russia both say they want peace. E...\n",
       "18  Sanctioned Western tech is still entering Russ...\n",
       "19  Bahamas PM fears climate action could be sidel...\n",
       "20  Earth close to breaching 1.5 degrees Celsius a...\n",
       "21  Norway poised to open vast ocean area to contr...\n",
       "22  Oil industry veteran to lead next round of UN ...\n",
       "23  CNBCâ€™s Sustainable Future Forum 2023: Watch th...\n",
       "24  Southeast Asia is on the cusp of a 'supercharg...\n",
       "25  Cambodia's deputy prime minister: BRI has help...\n",
       "26  Southeast Asia's first luxury hotel made from ...\n",
       "27  Ahead of Indonesiaâ€™s elections, critics slam J...\n",
       "28  Laos is spiraling toward a debt crisis as Chin...\n",
       "29  Americans are canceling trips that are thousan...\n",
       "30  Social media uproar may cost the Maldives mill...\n",
       "31  An new train pass is launching in Japan â€” but ...\n",
       "32  Will air fares remain high in 2024? Here's wha...\n",
       "33  Why foreign workers are flocking to this 700-y...\n",
       "34  You are 48% more likely to land a first date i...\n",
       "35  How 27-year-old whose business brought in $1 m...\n",
       "36  The 'holy grail' of longevity foods this docto...\n",
       "37  The No. 1 in-demand remote job companies are h...\n",
       "38  The ultimate guide to acing your interview and..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5. Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all anchor tags with the specified class for headlines\n",
    "headlines = soup.find_all('a', class_='Card-title')\n",
    "\n",
    "# Extract headlines text and create a DataFrame\n",
    "headlines_list = [headline.text.strip() for headline in headlines]\n",
    "df = pd.DataFrame({'Headline': headlines_list})\n",
    "\n",
    "# Print the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "90b627c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knowledge graphs as tools for explainable mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Explainable AI tools for legal reasoning about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hard choices in artificial intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Assessing the communication gap between AI mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Artificial cognition for social humanâ€“robot in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The multifaceted impact of Ada Lovelace in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Robot ethics: Mapping the issues for a mechani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Reward (Mis)design for autonomous driving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Planning and acting in partially observable st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What do we want from Explainable Artificial In...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title\n",
       "0                                    Reward is enough\n",
       "1   Explanation in artificial intelligence: Insigh...\n",
       "2              Creativity and artificial intelligence\n",
       "3   Conflict-based search for optimal multi-agent ...\n",
       "4   Knowledge graphs as tools for explainable mach...\n",
       "5   Law and logic: A review from an argumentation ...\n",
       "6   Between MDPs and semi-MDPs: A framework for te...\n",
       "7   Explaining individual predictions when feature...\n",
       "8       Multiple object tracking: A literature review\n",
       "9   A survey of inverse reinforcement learning: Ch...\n",
       "10  Evaluating XAI: A comparison of rule-based and...\n",
       "11  Explainable AI tools for legal reasoning about...\n",
       "12            Hard choices in artificial intelligence\n",
       "13  Assessing the communication gap between AI mod...\n",
       "14  Explaining black-box classifiers using post-ho...\n",
       "15  The Hanabi challenge: A new frontier for AI re...\n",
       "16              Wrappers for feature subset selection\n",
       "17  Artificial cognition for social humanâ€“robot in...\n",
       "18  A review of possible effects of cognitive bias...\n",
       "19  The multifaceted impact of Ada Lovelace in the...\n",
       "20  Robot ethics: Mapping the issues for a mechani...\n",
       "21          Reward (Mis)design for autonomous driving\n",
       "22  Planning and acting in partially observable st...\n",
       "23  What do we want from Explainable Artificial In..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details \n",
    "and make data frame-'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all anchor tags with the specified class for headlines\n",
    "titles = soup.find_all('h2', class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\")\n",
    "\n",
    "# Extracting Titles\n",
    "titles_list = [title.text.strip() for title in titles]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Title': titles_list})\n",
    "\n",
    "# Print the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e0f53a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Silver, Satinder Singh, Doina Precup, Ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tim Miller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Margaret A. Boden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guni Sharon, Roni Stern, Ariel Felner, Nathan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ilaria Tiddi, Stefan Schlobach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Henry Prakken, Giovanni Sartor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Richard S. Sutton, Doina Precup, Satinder Singh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kjersti Aas, Martin Jullum, Anders LÃ¸land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wenhan Luo, Junliang Xing and 4 more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Saurabh Arora, Prashant Doshi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jasper van der Waa, Elisabeth Nieuwburg, Anita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Joe Collenette, Katie Atkinson, Trevor Bench-C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Oskar Wysocki, Jessica Katharine Davies and 5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nolan Bard, Jakob N. Foerster and 13 more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ron Kohavi, George H. John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SÃ©verin Lemaignan, Mathieu Warnier and 3 more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TomÃ¡Å¡ Kliegr, Å tÄ›pÃ¡n BahnÃ­k, Johannes FÃ¼rnkranz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Luigia Carlucci Aiello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Patrick Lin, Keith Abney, George Bekey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>W. Bradley Knox, Alessandro Allievi and 3 more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Leslie Pack Kaelbling, Michael L. Littman, Ant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Markus Langer, Daniel Oster and 6 more</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Authors\n",
       "0   David Silver, Satinder Singh, Doina Precup, Ri...\n",
       "1                                          Tim Miller\n",
       "2                                   Margaret A. Boden\n",
       "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...\n",
       "4                      Ilaria Tiddi, Stefan Schlobach\n",
       "5                      Henry Prakken, Giovanni Sartor\n",
       "6     Richard S. Sutton, Doina Precup, Satinder Singh\n",
       "7           Kjersti Aas, Martin Jullum, Anders LÃ¸land\n",
       "8                Wenhan Luo, Junliang Xing and 4 more\n",
       "9                       Saurabh Arora, Prashant Doshi\n",
       "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...\n",
       "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...\n",
       "12   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz\n",
       "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...\n",
       "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...\n",
       "15          Nolan Bard, Jakob N. Foerster and 13 more\n",
       "16                         Ron Kohavi, George H. John\n",
       "17      SÃ©verin Lemaignan, Mathieu Warnier and 3 more\n",
       "18    TomÃ¡Å¡ Kliegr, Å tÄ›pÃ¡n BahnÃ­k, Johannes FÃ¼rnkranz\n",
       "19                             Luigia Carlucci Aiello\n",
       "20             Patrick Lin, Keith Abney, George Bekey\n",
       "21     W. Bradley Knox, Alessandro Allievi and 3 more\n",
       "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...\n",
       "23             Markus Langer, Daniel Oster and 6 more"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all span tags with the specified class for authors\n",
    "authors = soup.find_all('span', class_=\"sc-1w3fpd7-0 dnCnAO\")\n",
    "\n",
    "# Extracting Authors\n",
    "authors_list = [author.text.strip() for author in authors]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Authors': authors_list})\n",
    "\n",
    "# Print the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de9ca35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>October 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>February 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>August 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>February 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>January 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>October 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>August 1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>September 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>April 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>August 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>February 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>April 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>November 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>March 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>May 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>March 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>December 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>June 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>June 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>June 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>April 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>March 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>May 1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>July 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time\n",
       "0     October 2021\n",
       "1    February 2019\n",
       "2      August 1998\n",
       "3    February 2015\n",
       "4     January 2022\n",
       "5     October 2015\n",
       "6      August 1999\n",
       "7   September 2021\n",
       "8       April 2021\n",
       "9      August 2021\n",
       "10   February 2021\n",
       "11      April 2023\n",
       "12   November 2021\n",
       "13      March 2023\n",
       "14        May 2021\n",
       "15      March 2020\n",
       "16   December 1997\n",
       "17       June 2017\n",
       "18       June 2021\n",
       "19       June 2016\n",
       "20      April 2011\n",
       "21      March 2023\n",
       "22        May 1998\n",
       "23       July 2021"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all p tags with the specified class for publication times\n",
    "times = soup.find_all('span', class_=\"sc-1thf9ly-2 dvggWt\")\n",
    "\n",
    "# Extracting Times\n",
    "time_list = [time.text.strip() for time in times]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Publication Time': time_list})\n",
    "\n",
    "# Print the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "17ef0646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Paper URL\n",
      "0   https://www.sciencedirect.com/science/article/...\n",
      "1   https://www.sciencedirect.com/science/article/...\n",
      "2   https://www.sciencedirect.com/science/article/...\n",
      "3   https://www.sciencedirect.com/science/article/...\n",
      "4   https://www.sciencedirect.com/science/article/...\n",
      "5   https://www.sciencedirect.com/science/article/...\n",
      "6   https://www.sciencedirect.com/science/article/...\n",
      "7   https://www.sciencedirect.com/science/article/...\n",
      "8   https://www.sciencedirect.com/science/article/...\n",
      "9   https://www.sciencedirect.com/science/article/...\n",
      "10  https://www.sciencedirect.com/science/article/...\n",
      "11  https://www.sciencedirect.com/science/article/...\n",
      "12  https://www.sciencedirect.com/science/article/...\n",
      "13  https://www.sciencedirect.com/science/article/...\n",
      "14  https://www.sciencedirect.com/science/article/...\n",
      "15  https://www.sciencedirect.com/science/article/...\n",
      "16  https://www.sciencedirect.com/science/article/...\n",
      "17  https://www.sciencedirect.com/science/article/...\n",
      "18  https://www.sciencedirect.com/science/article/...\n",
      "19  https://www.sciencedirect.com/science/article/...\n",
      "20  https://www.sciencedirect.com/science/article/...\n",
      "21  https://www.sciencedirect.com/science/article/...\n",
      "22  https://www.sciencedirect.com/science/article/...\n",
      "23  https://www.sciencedirect.com/science/article/...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Paper URL\n",
       "0   https://www.sciencedirect.com/science/article/...\n",
       "1   https://www.sciencedirect.com/science/article/...\n",
       "2   https://www.sciencedirect.com/science/article/...\n",
       "3   https://www.sciencedirect.com/science/article/...\n",
       "4   https://www.sciencedirect.com/science/article/...\n",
       "5   https://www.sciencedirect.com/science/article/...\n",
       "6   https://www.sciencedirect.com/science/article/...\n",
       "7   https://www.sciencedirect.com/science/article/...\n",
       "8   https://www.sciencedirect.com/science/article/...\n",
       "9   https://www.sciencedirect.com/science/article/...\n",
       "10  https://www.sciencedirect.com/science/article/...\n",
       "11  https://www.sciencedirect.com/science/article/...\n",
       "12  https://www.sciencedirect.com/science/article/...\n",
       "13  https://www.sciencedirect.com/science/article/...\n",
       "14  https://www.sciencedirect.com/science/article/...\n",
       "15  https://www.sciencedirect.com/science/article/...\n",
       "16  https://www.sciencedirect.com/science/article/...\n",
       "17  https://www.sciencedirect.com/science/article/...\n",
       "18  https://www.sciencedirect.com/science/article/...\n",
       "19  https://www.sciencedirect.com/science/article/...\n",
       "20  https://www.sciencedirect.com/science/article/...\n",
       "21  https://www.sciencedirect.com/science/article/...\n",
       "22  https://www.sciencedirect.com/science/article/...\n",
       "23  https://www.sciencedirect.com/science/article/..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find all a tags within the specified class for paper URLs\n",
    "paper_urls = soup.find_all('a', class_='sc-5smygv-0 fIXTHm')\n",
    "\n",
    "# Extracting Paper URLs\n",
    "paper_url_list = [paper_url['href'] for paper_url in paper_urls]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Paper URL': paper_url_list})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5754039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
